{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and plotting some binary classification training sets with two features\n",
    "### binary_dataset_galery : Generate some binary classification training sets with two features\n",
    "### plot_gallery : Plot the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates 25 datasets for binary classification\n",
    "# These datasets are suitable for one class SVM\n",
    "\n",
    "def binary_dataset_galery(n_samples=300,noise=0):\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from sklearn import svm\n",
    "    from sklearn.datasets import make_moons, make_blobs,make_classification\n",
    "    from sklearn.covariance import EllipticEnvelope\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    from sklearn.linear_model import SGDOneClassSVM\n",
    "    from sklearn.kernel_approximation import Nystroem\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.datasets import make_classification, make_blobs, make_circles,make_moons\n",
    "\n",
    "  \n",
    "\n",
    "    datasets = [\n",
    "            make_blobs(n_samples=100,  n_features=2, centers=2, cluster_std=2, center_box=(-10.0, 10.0), shuffle=False, random_state=1, return_centers=False),\n",
    "            make_classification(n_samples=150, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, class_sep=2.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=20),\n",
    "            make_classification(n_samples=150, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=2, class_sep=2.5, random_state=42),\n",
    "            make_classification(n_samples=150, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, class_sep=2.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=42),\n",
    "            make_blobs(n_samples=200,  n_features=2, centers=2, cluster_std=3, center_box=(-10.0, 10.0), shuffle=False, random_state=1, return_centers=False),\n",
    "            make_blobs(n_samples=150,  n_features=2, centers=2, cluster_std=4, center_box=(-10.0, 10.0), shuffle=False, random_state=1, return_centers=False),\n",
    "            make_classification(n_samples=300, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, class_sep=2.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=42),\n",
    "            make_classification(n_samples=300, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, class_sep=2.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=20),\n",
    "            make_classification(n_samples=300, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=2, class_sep=2.5, random_state=42),\n",
    "            make_classification(n_samples=300, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, class_sep=1.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=20),\n",
    "            make_classification(n_samples=300, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, class_sep=2.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=42),       \n",
    "            make_circles(n_samples=100, shuffle=False, noise=None, random_state=1, factor=0.5),\n",
    "            make_circles(n_samples=100, shuffle=False, noise=None, random_state=1, factor=0.7),\n",
    "            make_circles(n_samples=100, shuffle=False, noise=None, random_state=1, factor=0.9),\n",
    "            make_circles(n_samples=100, shuffle=False, noise=.1, random_state=1, factor=0.5),\n",
    "            make_circles(n_samples=100, shuffle=False, noise=.02, random_state=1, factor=0.9),\n",
    "            make_moons(n_samples=100,shuffle=False, noise=0, random_state=1),\n",
    "            make_moons(n_samples=150,shuffle=False, noise=0.1, random_state=1),\n",
    "            make_moons(n_samples=150,shuffle=False, noise=0.2, random_state=1)\n",
    "        ]\n",
    "\n",
    "\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = dataset\n",
    "        labels.append(y)\n",
    "        features.append(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Xt,yt=make_classification(n_samples, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, class_sep=2.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=42)\n",
    "    labels.append(yt)\n",
    "    features.append(Xt)\n",
    "\n",
    "    Xt,yt=make_classification(n_samples, flip_y=noise, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, class_sep=2.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=42)\n",
    "    labels.append(yt)\n",
    "    features.append(Xt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---> One classes SVM..................................................................................................\n",
    "    # Example settings\n",
    "    outliers_fraction = 0.15\n",
    "    n_outliers = int(outliers_fraction * n_samples)\n",
    "    n_inliers = n_samples - n_outliers\n",
    "\n",
    "    # define outlier/anomaly detection methods to be compared.\n",
    "    # the SGDOneClassSVM must be used in a pipeline with a kernel approximation\n",
    "    # to give similar results to the OneClassSVM\n",
    "    anomaly_algorithms = [\n",
    "        (\n",
    "            \"Robust covariance\",\n",
    "            EllipticEnvelope(contamination=outliers_fraction, random_state=42),\n",
    "        ),\n",
    "        (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.1)),\n",
    "        (\n",
    "            \"One-Class SVM (SGD)\",\n",
    "            make_pipeline(\n",
    "                Nystroem(gamma=0.1, random_state=42, n_components=150),\n",
    "                SGDOneClassSVM(\n",
    "                    nu=outliers_fraction,\n",
    "                    shuffle=True,\n",
    "                    fit_intercept=True,\n",
    "                    random_state=42,\n",
    "                    tol=1e-6,\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"Isolation Forest\",\n",
    "            IsolationForest(contamination=outliers_fraction, random_state=42),\n",
    "        ),\n",
    "        (\n",
    "            \"Local Outlier Factor\",\n",
    "            LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Define datasets\n",
    "    blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
    "    datasets = [\n",
    "        make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],\n",
    "        make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],\n",
    "        make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0],\n",
    "        4.0\n",
    "        * (\n",
    "            make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0]\n",
    "            - np.array([0.5, 0.25])\n",
    "        ),\n",
    "        14.0 * (np.random.RandomState(42).rand(n_samples, 2) - 0.5),\n",
    "    ]\n",
    "\n",
    "    # Compare given classifiers under given settings\n",
    "    xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n",
    "\n",
    "    \n",
    "    rng = np.random.RandomState(42)\n",
    "\n",
    " \n",
    "    for i_dataset, X in enumerate(datasets):\n",
    "        # Add outliers\n",
    "        X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\n",
    "\n",
    "        for name, algorithm in anomaly_algorithms:\n",
    "            t0 = time.time()\n",
    "            algorithm.fit(X)\n",
    "            t1 = time.time()\n",
    "     \n",
    "            # fit the data and tag outliers\n",
    "            if name == \"Local Outlier Factor\":\n",
    "                y_pred = algorithm.fit_predict(X)\n",
    "            else:\n",
    "                y_pred = algorithm.fit(X).predict(X)\n",
    "\n",
    "\n",
    "            labels.append((y_pred+1)/2)\n",
    "            features.append(X)\n",
    "            #datasets.append([X,y_pred])\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "    \n",
    "    n1=50\n",
    "    r=2*np.random.random((n1,))\n",
    "    t=2*np.pi*np.random.random((n1,))\n",
    "    Xc1=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y1=np.ones(n1,dtype=int)\n",
    "\n",
    "    n2=100\n",
    "    r=3+2*np.random.random((n2,))\n",
    "    t=2*np.pi*np.random.random((n2,))\n",
    "    Xc2=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y2=np.zeros(n2,dtype=int)\n",
    "\n",
    "    n3=150\n",
    "    r=6+2*np.random.random((n3,))\n",
    "    t=2*np.pi*np.random.random((n3,))\n",
    "    Xc3=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y3=np.ones(n3,dtype=int)\n",
    "\n",
    "    Xc=np.concatenate([Xc1,Xc2,Xc3])\n",
    "    yc=np.concatenate([y1,y2,y3])\n",
    "\n",
    "    labels.append(yc)\n",
    "    features.append(Xc)\n",
    "\n",
    "\n",
    "    #-----\n",
    "    \n",
    "    \n",
    "    n1=50\n",
    "    r=2*np.random.random((n1,))\n",
    "    t=2*np.pi*np.random.random((n1,))\n",
    "    Xc1=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y1=np.ones(n1,dtype=int)\n",
    "\n",
    "    n2=100\n",
    "    r=2.5+2*np.random.random((n2,))\n",
    "    t=2*np.pi*np.random.random((n2,))\n",
    "    Xc2=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y2=np.zeros(n2,dtype=int)\n",
    "\n",
    "    n3=150\n",
    "    r=5+2*np.random.random((n3,))\n",
    "    t=2*np.pi*np.random.random((n3,))\n",
    "    Xc3=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y3=np.ones(n3,dtype=int)\n",
    "\n",
    "    Xc=np.concatenate([Xc1,Xc2,Xc3])\n",
    "    yc=np.concatenate([y1,y2,y3])\n",
    "\n",
    "    labels.append(yc)\n",
    "    features.append(Xc)\n",
    "\n",
    "\n",
    "    #-----\n",
    "\n",
    "       \n",
    "    n1=50\n",
    "    r=2*np.random.random((n1,))\n",
    "    t=2*np.pi*np.random.random((n1,))\n",
    "    Xc1=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y1=np.ones(n1,dtype=int)\n",
    "\n",
    "    n2=100\n",
    "    r=3+2*np.random.random((n2,))\n",
    "    t=2*np.pi*np.random.random((n2,))\n",
    "    Xc2=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y2=np.zeros(n2,dtype=int)\n",
    "\n",
    "    n3=150\n",
    "    r=6+3*np.random.random((n3,))\n",
    "    t=2*np.pi*np.random.random((n3,))\n",
    "    Xc3=np.array([r*np.cos(t),r*np.sin(t)]).transpose()\n",
    "    y3=np.ones(n3,dtype=int)\n",
    "\n",
    "    Xc=np.concatenate([Xc1-5,Xc2-5,Xc3-5,Xc1+5,Xc2+5,Xc3+5])\n",
    "    yc=np.concatenate([y1,y2,y3,y1,y2,y3])\n",
    "\n",
    "    labels.append(yc)\n",
    "    features.append(Xc)\n",
    "\n",
    "\n",
    "    #-----\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "    Xc=10*np.random.random((400,2))-5\n",
    "    dc=(Xc**2).sum(axis=1)\n",
    "    yc=np.ones(len(dc),dtype=int)\n",
    "    yc[dc<2]=0\n",
    "    yc[dc>10]=0\n",
    "    labels.append(yc)\n",
    "    features.append(Xc)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return features,labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each dataset in a separate subplot  \n",
    "def plot_gallery(XX,yy,n_cols=4,fsize=20):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    n_rows=int(np.ceil(len(XX)/n_cols))\n",
    "    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(fsize,fsize*n_rows/n_cols))\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    for i in range(len(XX)):\n",
    "        X=XX[i]\n",
    "        y=yy[i]\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        Xa=X[y==0,:]\n",
    "        axs[row, col].scatter(Xa[:,0],Xa[:,1],color=[1,0,0],label='Class 0')\n",
    "\n",
    "        Xb=X[y==1,:]\n",
    "        axs[row, col].scatter(Xb[:,0],Xb[:,1],color=[0,0,1],label='Class 1')\n",
    "        axs[row, col].legend()\n",
    "        axs[row, col].grid()\n",
    "        axs[row, col].set_title(f\"Dataset {i+1}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
